{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_tutorial.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# PyTorch チュートリアル\n","本テキストでは、PyTorchの基本文法について説明します。PyTorchには特有のテンソル型があり、基本的にこのテンソル型を用いて演算を行います。テンソルは、NumPyのndarrayと類似していますが、GPUによる高速演算が可能であるという特徴があります。また、GPUを用いることで計算効率を向上させられますが、GPUの使用を明示的に指定する必要があるといった特徴があります。本テキストでは、上記の事柄を中心に説明します。"],"metadata":{"id":"k87BkGDfzFQi"}},{"cell_type":"markdown","source":["# 目次\n","1. テンソル:Tensorsとは\n","1. 基本演算\n","2. TensorとNumPyの相互変換\n","  - TensorからNumPyへの変換\n","  - TensorからNumPyへの変換\n","3. GPUとCPUの使い分け"],"metadata":{"id":"Ojqjfx9sepIw"}},{"cell_type":"markdown","metadata":{"id":"qP43GdSypjIh"},"source":["# 1. テンソル：Tensorsとは\n","\n","テンソルは特殊なデータ構造で、配列や行列によく似ています。\n","\n","PyTorchではテンソル型の変数を使用して、モデルの入力と出力、そしてモデルのパラメータを表現します。\n"]},{"cell_type":"markdown","metadata":{"id":"k5c9yTQpp9pP"},"source":["テンソルは[NumPy](https://numpy.org/)のndarraysに似ていますが、GPU利用による高速化が可能で、本資料での解説は行いませんが「自動微分」用のパラメータが追加されている点で異なっています。\n","\n","テンソルとNumPyの配列は基本的には同じメモリを共有することができるため、2つの型間での変換時にはデータをコピーする必要がありません。\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mczXVnIbqslq"},"source":["\n","\n","NumPyのndarraysに慣れている人は、Tensor APIをすぐに使いこなせると思います。\n","\n","そうでない場合には、本チュートリアルを通してぜひ習得してください。"]},{"cell_type":"markdown","source":["# 2. 基本演算"],"metadata":{"id":"o6p1U5EYyT5w"}},{"cell_type":"code","metadata":{"id":"6oOJ0Pj1o65h"},"source":["import torch\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qNpNfUJDqwhg"},"source":["## テンソルの初期化\n","\n","\n","テンソルは様々な手法で初期化できます。\n","\n","以下に例を示します。\n","\n"]},{"cell_type":"markdown","source":["**テンソルの作成**\n","\n","テンソルを作成するには、次のようにしますが、この方法では中身は初期化されていないことに注意してください。"],"metadata":{"id":"emBI-0N_89Cf"}},{"cell_type":"code","source":["import torch\n","x = torch.empty(5, 3)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLWodMCz9M-u","executionInfo":{"status":"ok","timestamp":1650876515283,"user_tz":-540,"elapsed":13,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"d172c836-e90b-4591-b90b-137d569fa0a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[4.7799e-22, 3.0854e-41, 3.3631e-44],\n","        [0.0000e+00,        nan, 3.0854e-41],\n","        [1.1578e+27, 1.1362e+30, 7.1547e+22],\n","        [4.5828e+30, 1.2121e+04, 7.1846e+22],\n","        [9.2198e-39, 7.0374e+22, 8.9205e-23]])"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"gD6xIZDhrEvF"},"source":["**データから直接テンソルに変換**\n","\n","データから直接テンソルを作ることができます。\n","\n","その際、データ型は自動的に推測されます。\n"]},{"cell_type":"code","metadata":{"id":"1Zo-5JUco65i"},"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0i4Re_DCo65j"},"source":["**NumPy arrayからテンソルに変換**\n","\n","テンソルとNumpy arraysは相互に変換可能です。\n","\n"]},{"cell_type":"code","metadata":{"id":"SErVsUQmo65k"},"source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YIwtzI63o65k"},"source":["**他のテンソルから作成**\n","\n","他のテンソルから新しいテンソルを作成する場合、明示的に上書きされない限り、引数のテンソルのプロパティ（形状、データ型）を保持します。その場合、_likeがつく関数を使用します。\n"]},{"cell_type":"code","metadata":{"id":"W9AX5RQZo65l","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f6e2d9b-c8c2-42dc-dc7b-498639b49807","executionInfo":{"status":"ok","timestamp":1650876515887,"user_tz":-540,"elapsed":58,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}}},"source":["x_ones = torch.ones_like(x_data) # x_dataの特性（プロパティ）を維持\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # x_dataのdatatypeを上書き更新\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.6887, 0.9360],\n","        [0.9095, 0.8902]]) \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"smW9ouQZo65l"},"source":["**ランダム値や定数のテンソルの作成**\n","\n","\n","``shape``は、テンソルの次元を示すタプルです。\n","\n","以下の例では、shapeからテンソルのサイズを決めています。\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"nrHFyTYCo65m","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1249a4b9-d607-4135-ed11-f8043b86b682","executionInfo":{"status":"ok","timestamp":1650876515888,"user_tz":-540,"elapsed":57,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}}},"source":["shape = (2,3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Tensor: \n"," tensor([[0.8197, 0.7803, 0.5036],\n","        [0.5415, 0.5585, 0.6357]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}]},{"cell_type":"code","source":["x = torch.tensor([4, 3])\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65EMFjwPC1s8","executionInfo":{"status":"ok","timestamp":1650876515889,"user_tz":-540,"elapsed":55,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"4af7e61f-5e87-471d-d9f8-bc5289533661"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([4, 3])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["大きさを知るには、sizeメソッドを呼び出します。"],"metadata":{"id":"CFV5NKMPXG7b"}},{"cell_type":"code","source":["x.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nqtJGmdnXRYB","executionInfo":{"status":"ok","timestamp":1650876515890,"user_tz":-540,"elapsed":53,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"03ab3ef0-c5b8-4389-869a-290ff302354e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["型を知るだけであれば、typeメソッドを使用します。"],"metadata":{"id":"_sTzCDmHXs9P"}},{"cell_type":"code","source":["type(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrlpr6OqZD9C","executionInfo":{"status":"ok","timestamp":1650876515892,"user_tz":-540,"elapsed":52,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"d4e887a3-393b-467d-db13-5c2ecca454e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["整数を要素に持つテンソルと、浮動小数点を要素に持つテンソルをそれぞれ作成して違いを確認しましょう。\n"],"metadata":{"id":"nEQgbjsmZ_co"}},{"cell_type":"code","source":["x = torch.tensor([1, 2, 3, 4, 5, 6])\n","x.type()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"h9V_HsPQcKdi","executionInfo":{"status":"ok","timestamp":1650876515893,"user_tz":-540,"elapsed":51,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"cb0f8d00-9713-436f-cba8-e4111c7a74cd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'torch.LongTensor'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["x = torch.FloatTensor([1, 2, 3, 4, 5, 6])\n","x.type()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Jv41RDDQd8a3","executionInfo":{"status":"ok","timestamp":1650876515894,"user_tz":-540,"elapsed":50,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"e8004c4d-8767-4087-e1b3-9aded7852d3b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'torch.FloatTensor'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["次の大文字Tensorと小文字tensorの違いに注意してください。\n","大文字Tensorは浮動小数点型、小文字tensorは整数型です。"],"metadata":{"id":"qdkmmPMAeIaY"}},{"cell_type":"code","source":["x = torch.Tensor([1, 2, 3, 4, 5, 6])\n","x.type()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"OJU07ei4cqiF","executionInfo":{"status":"ok","timestamp":1650876515896,"user_tz":-540,"elapsed":49,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"9fe71507-66d0-47f8-c760-342511b0736a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'torch.FloatTensor'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"lX4udb5ao65n"},"source":["## テンソルの属性変数\n","\n","\n","\n","テンソルは属性変数として、その形状、データの型、保存されているデバイスを保持しています。\n"]},{"cell_type":"code","metadata":{"id":"iXMqBDEXo65n","colab":{"base_uri":"https://localhost:8080/"},"outputId":"01a41691-8a71-452f-caad-5f0503698b79","executionInfo":{"status":"ok","timestamp":1650876515898,"user_tz":-540,"elapsed":50,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}}},"source":["tensor = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}]},{"cell_type":"markdown","metadata":{"id":"kfBIS-6Eo65o"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z6hnnVOCo65o"},"source":["## テンソルの操作\n","\n","\n","PyTorchでは、算術、線形代数、行列操作（転置、インデックス、スライス）など、100種類以上のテンソル演算が可能です。フーリエ変換、逆行列など多くの演算が可能となっています。\n","\n","種々操作の詳細は[こちら](https://pytorch.org/docs/stable/torch.html)\n","をご覧ください。\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"syaRBPBho65p"},"source":["テンソル操作の中からいくつかを試してみましょう。\n","\n","テンソルの操作方法は、NumPyの操作方法に類似しているため、NumPyに慣れている人は、テンソルの操作も容易かと思います。\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7u3kTw8to65p"},"source":["**インデックスとスライス**\n","\n"]},{"cell_type":"code","metadata":{"id":"1OF85To5o65p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad963988-e797-4bc8-bd54-8343801bb6db","executionInfo":{"status":"ok","timestamp":1650876515899,"user_tz":-540,"elapsed":48,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}}},"source":["tensor = torch.ones(4, 4)\n","print('First row: ',tensor[0])\n","print('First column: ', tensor[:, 0])\n","print('Last column:', tensor[..., -1])\n","tensor[:,1] = 0\n","print(tensor)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First row:  tensor([1., 1., 1., 1.])\n","First column:  tensor([1., 1., 1., 1.])\n","Last column: tensor([1., 1., 1., 1.])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}]},{"cell_type":"markdown","source":["numpyと同様の方法で、行を入手するには以下のように記述します。"],"metadata":{"id":"vyShT_t6Ghlx"}},{"cell_type":"code","source":["x = torch.rand(5, 3)\n","print(x) \n","print(x[1]) # 2行めの入手"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SR6rmwNQGThA","executionInfo":{"status":"ok","timestamp":1650876515900,"user_tz":-540,"elapsed":46,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"310990bf-6e9a-40af-a678-d96c994f7623"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4406, 0.2965, 0.9079],\n","        [0.6862, 0.3601, 0.8544],\n","        [0.7490, 0.6419, 0.7057],\n","        [0.2902, 0.0737, 0.6788],\n","        [0.9286, 0.5344, 0.4114]])\n","tensor([0.6862, 0.3601, 0.8544])\n"]}]},{"cell_type":"markdown","source":["同じように、列の入手は以下のように記述します。"],"metadata":{"id":"4aJqo2zjGxwz"}},{"cell_type":"code","source":["x = torch.rand(5, 3)\n","print(x) \n","print(x[:,1]) #2列めの入手"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00JZMZYvG5j6","executionInfo":{"status":"ok","timestamp":1650876515902,"user_tz":-540,"elapsed":46,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"b91e1200-c96d-4394-d2a8-7ab674d1b136"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.7546, 0.4525, 0.0076],\n","        [0.8726, 0.2029, 0.3650],\n","        [0.9411, 0.5154, 0.3957],\n","        [0.1645, 0.4130, 0.9437],\n","        [0.0831, 0.4594, 0.5817]])\n","tensor([0.4525, 0.2029, 0.5154, 0.4130, 0.4594])\n"]}]},{"cell_type":"markdown","source":["部分要素を入手する場合には以下のように記述します。"],"metadata":{"id":"cn3_VqnfHhh9"}},{"cell_type":"code","source":["x = torch.rand(5, 5)\n","print(x)\n","x[2:4,1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cuMWGXz2G5XY","executionInfo":{"status":"ok","timestamp":1650876515905,"user_tz":-540,"elapsed":46,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"17aab880-5f28-4ddc-fea4-6885416e2ee0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2689, 0.5732, 0.1335, 0.1412, 0.1294],\n","        [0.6634, 0.3808, 0.4097, 0.9845, 0.6333],\n","        [0.1351, 0.2406, 0.4529, 0.7092, 0.9501],\n","        [0.1822, 0.6359, 0.6850, 0.9461, 0.2541],\n","        [0.7246, 0.6324, 0.2945, 0.8066, 0.6736]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([0.2406, 0.6359])"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["**行列のリサイズ**\n","\n","PyTorchでテンソルを操作する関数は、transpose, view, reshapeがあります。\n"],"metadata":{"id":"9wOzdZY7Htyg"}},{"cell_type":"markdown","source":["基本はtransposeでテンソルを転置する関数となっています。"],"metadata":{"id":"3Pn_CjB7KkMO"}},{"cell_type":"code","source":["x = torch.rand(4, 3)\n","print(x)\n","y = torch.transpose(x, 0, 1) # 転置している\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_VSu_uTKKP9","executionInfo":{"status":"ok","timestamp":1650876515906,"user_tz":-540,"elapsed":43,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"084ec1ae-e2ba-45ef-d8e1-a3ed79c2be59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2573, 0.1386, 0.6858],\n","        [0.5918, 0.8789, 0.4690],\n","        [0.2013, 0.9312, 0.3803],\n","        [0.9897, 0.3238, 0.2955]])\n","tensor([[0.2573, 0.5918, 0.2013, 0.9897],\n","        [0.1386, 0.8789, 0.9312, 0.3238],\n","        [0.6858, 0.4690, 0.3803, 0.2955]])\n"]}]},{"cell_type":"markdown","source":["なお、transposeは見え方のみ変える関数で、データの入れ替えは行いません。\n","\n","参考：TensorはStorageという名の一次元配列でデータを保持しており、それをoffset, strideという変数の値を用いて行列のように見せています。見え方を変えるとはoffset, strideを変更することで、データの入れ替えとはStorageを変更することを言います。[参考資料](https://ohke.hateblo.jp/entry/2019/11/30/230000)\n"],"metadata":{"id":"ksd4kMs6e9Yx"}},{"cell_type":"markdown","source":["view(y, x)で、y行x列のデータに変換することができます。\n","  - -1をどちらかに指定すると自動的に大きさから計算してくれますが、約数でないと変換できずエラーになります。\n","  - viewはデータの入れ替えは行わず見え方のみ操作する関数で、Storage上の並びは変更されません。"],"metadata":{"id":"whh-hYDAMwG4"}},{"cell_type":"code","source":["x = torch.randn(4, 4) # 4×4の乱数行列を作成\n","y = x.view(16)  # 16次元の行列にサイズ変更\n","z = x.view(-1, 8) # -1を使うと、サイズが自動調整\n","print(x.size(), y.size(), z.size())\n","print(x, \"\\n\", y, \"\\n\", z, \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUoRHDbnIUHb","executionInfo":{"status":"ok","timestamp":1650876515909,"user_tz":-540,"elapsed":44,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"7ffafa72-1799-4705-b4db-ba35796e7813"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n","tensor([[ 0.6498,  1.4602, -0.7937, -0.7266],\n","        [ 1.7468,  0.2590,  0.3075, -0.5896],\n","        [ 1.2868, -0.3919, -0.6544, -0.8888],\n","        [-0.5944, -0.5710, -0.6808,  0.0649]]) \n"," tensor([ 0.6498,  1.4602, -0.7937, -0.7266,  1.7468,  0.2590,  0.3075, -0.5896,\n","         1.2868, -0.3919, -0.6544, -0.8888, -0.5944, -0.5710, -0.6808,  0.0649]) \n"," tensor([[ 0.6498,  1.4602, -0.7937, -0.7266,  1.7468,  0.2590,  0.3075, -0.5896],\n","        [ 1.2868, -0.3919, -0.6544, -0.8888, -0.5944, -0.5710, -0.6808,  0.0649]]) \n","\n"]}]},{"cell_type":"markdown","source":["ただ、viewは一つだけ注意点があります。\n","それは、先述したように、viewは見え方のみ操作する関数なので、viewを適用するTensorのStorageオブジェクトの要素は、行列の左上から見て連続な要素順に並んでいる必要があります。\n","\n","例えば、転置したTensorに対してviewでサイズ数を変更したい場合、そのまま実行すると下記のようにエラーになります。transposeにより見え方のみ変更されて、行列に対するStorageの並びが不連続となっているときに、再度見え方のみ変更しようとしてしまうからです。"],"metadata":{"id":"EqRPmxgALFgH"}},{"cell_type":"code","source":["torch.transpose(x, 0, 1).view(-1, 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"sysbZ1-3LFKQ","executionInfo":{"status":"error","timestamp":1650876516843,"user_tz":-540,"elapsed":975,"user":{"displayName":"朽木瑛","userId":"13542439018168855072"}},"outputId":"82597d2a-5962-4f17-c4c0-72322f7c29f2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-c8ae5abc29a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."]}]},{"cell_type":"markdown","source":["したがって、以下のように、viewの前にcontiguous()を呼び出すとStorageの要素が行列に対して連続になるように並び替えられ、そのあとにviewすると回避できます。"],"metadata":{"id":"HTpMv-eYLy4z"}},{"cell_type":"code","source":["torch.transpose(x, 0, 1).contiguous().view(-1, 2)"],"metadata":{"id":"gnG81TlQLyKU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["reshapeは、viewと同様にテンソルのリサイズができ、contiguous関数を必要としません。この理由は、データのコピーを作っている（=Storageを生成しなおす）からで、メモリ自体は消費しますが、エラーにはなりません。"],"metadata":{"id":"Gj01sGM9MEqz"}},{"cell_type":"code","source":["torch.transpose(x, 0, 1).reshape(-1, 2)"],"metadata":{"id":"32bDn9u6Mgvc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PyTorchテンソルの要素の型**\n","\n","テンソルの中に含める数値には、PyTorch独自のデータ型（`torch.dtypes`）が与えられていますが、あるテンソルに含まれる全要素は全て同じデータ型である必要があります。\n","\n","以下の型が準備されていますが、ほとんどの場合、`torch.float`か`torch.int`を用います。\n","\n","データ型 | dtype属性への記述 | 対応するPython／NumPy（np）のデータ型\n","---------|-----------------|--------------------------------\n","Boolean（真偽値）|torch.bool|bool／np.bool\n","8-bitの符号なし整数|torch.uint8|int／np.uint8\n","8-bitの符号付き整数|torch.int8|int／np.int8\n","16-bitの符号付き整数|torch.int16 ／ torch.short|int／np.uint16\n","32-bitの符号付き整数|torch.int32 ／ torch.int|int／np.uint32\n","64-bitの符号付き整数|torch.int64 ／ torch.long|int／np.uint64\n","16-bitの浮動小数点|torch.float16 ／ torch.half|float／np.float16\n","32-bitの浮動小数点|torch.float32 ／ torch.float|float／np.float32\n","64-bitの浮動小数点|torch.float64 ／ torch.double|float／np.float64\n"],"metadata":{"id":"eWslLYdEN6ad"}},{"cell_type":"markdown","metadata":{"id":"wHWwoi5wo65p"},"source":["**テンソルの結合** \n","\n","\n","``torch.cat``を使用することで、テンソルを特定の次元に沿って結合させることができます（詳細は[こちら](https://pytorch.org/docs/stable/generated/torch.stack.html)をご覧ください）。\n","\n","``torch.cat``とは微妙に異なるテンソル結合演算である[``torch.stack``](https://pytorch.org/docs/stable/generated/torch.stack.html)も確認しておいてください。\n","\n"]},{"cell_type":"code","metadata":{"id":"cS0A8qYao65q"},"source":["t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVuoj3lDo65q"},"source":["**算術演算**\n","\n"]},{"cell_type":"markdown","source":["**足し算の例**"],"metadata":{"id":"2JVKtK1zrCl7"}},{"cell_type":"code","source":["x = torch.ones(5, 3)\n","y = torch.ones(5, 3)\n","x+y"],"metadata":{"id":"a5Ogg9tHqoiZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["torchが準備するaddメソッドもありますが、演算子を用いたほうが分かりやすく、おすすめです。"],"metadata":{"id":"DqYERdyhtkrx"}},{"cell_type":"code","source":["torch.add(x, y)"],"metadata":{"id":"HiW0JiHwtxIq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["また、結果を格納するテンソルを準備して、addメソッドを使うことで出力先を指定することができますが、下に示す演算子を使った例のほうが分かりやすいかもしれません。"],"metadata":{"id":"jqzi2kixuncq"}},{"cell_type":"code","source":["result = torch.empty(5, 3)\n","torch.add(x, y, out=result)\n","result"],"metadata":{"id":"8ig1ftLatjZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.ones(5, 3)\n","y = torch.ones(5, 3)\n","y1 = y\n","y2 = y\n","y1.add(x)   # in-placeでないメソッド\n","y2 = y2 + x  # in-placeでないメソッド\n","print(y)     # yは変更されない\n","print(y1)\n","print(y2)"],"metadata":{"id":"61ZXiKCo8-0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["余談ですが、Pythonでは、in-placeのメソッドとin-placeでないメソッドがあります。下の`y2 += x`や`add_()`はin-placeですが、上の`y = y + x`や`add()`はin-placeでないメソッドです。\n","\n","これらの違いは、y1やy2に代入したyの値が、y1やy2の変更と一緒に変更されるかされないかの違いになっています。\n","\n","上と下の例で見比べてみましょう。"],"metadata":{"id":"rQkPnzyi7Mvc"}},{"cell_type":"code","source":["x = torch.ones(5, 3)\n","y = torch.ones(5, 3)\n","y1 = y\n","y2 = y\n","y1.add_(x)   # in-placeなメソッド\n","y2 += x      # in-placeなメソッド\n","print(y)     # yも変更される\n","print(y1)\n","print(y2)"],"metadata":{"id":"Fd_Gqik_vDMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["【注意】\n","\n","\n","in-place操作はメモリを節約できますが、演算履歴が失われてしまうため、微分を計算する際には問題となります。\n","\n","そのため、そのような微分を求める場面ではin-place操作の使用は推奨されていません。"],"metadata":{"id":"uRHZWk0ZEf4X"}},{"cell_type":"markdown","source":["**掛け算の例**\n","\n","2つのテンソル行列の掛け算です。y1,y2は同じ値になります。"],"metadata":{"id":"PNmlwBtzC6cO"}},{"cell_type":"code","source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)\n","data = [[2, 3],[4, 5]]\n","y_data = torch.tensor(data)\n","y1 = x_data @ y_data\n","y2 = x_data.matmul(y_data)\n","print(y1)\n","print(y2)"],"metadata":{"id":"98Tt8KeIrGcT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["こちらは要素ごとの積を求めます。z1,z2は同じ値になります。"],"metadata":{"id":"kXsNd_drDMDC"}},{"cell_type":"code","metadata":{"id":"L00SHkoMo65q"},"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)\n","data = [[2, 3],[4, 5]]\n","y_data = torch.tensor(data)\n","z1 = x_data * y_data\n","z2 = x_data.mul(y_data)\n","print(z1)\n","print(z2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCjumJ0uo65r"},"source":["**1要素のテンソル** \n","\n","1要素のテンソル（テンソルの全要素を足し算する等をした結果生まれます）を扱う場合には、``.item()``を使用することでPythonの数値型変数に変換できます。\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"TrA8pq1Jo65r"},"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)\n","agg = x_data.sum()\n","agg_item = agg.item()  \n","print(agg_item, type(agg_item))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. TensorとNumpyの相互変換\n","3章では、PyTorch特有のTensorと、Pythonで一般的に数式演算で用いられるNumPyの変換方法について説明します。<br>\n"],"metadata":{"id":"Kj0UHhMzewxs"}},{"cell_type":"markdown","source":["## TensorからNumPyへの変換\n","まず、TensorをNumPyに変換するには、numpy()メソッドを使えば簡単に変換できます。"],"metadata":{"id":"3SujkfUz-aSV"}},{"cell_type":"code","source":["import torch\n","x= torch.tensor([1,2,3,4,5,6])\n","print(x)\n","print(x.type())\n","y = x.numpy() #TensorをNumpyに変換\n","print(y)\n","print(type(y))"],"metadata":{"id":"cxBD1-IcfX5i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026073082,"user_tz":-540,"elapsed":1028,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"d8a4bef3-de9a-4d93-9dee-ab65db07c1b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3, 4, 5, 6])\n","torch.LongTensor\n","[1 2 3 4 5 6]\n","<class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","source":["尚、基本演算の部分と同様の議論になリますが、add_() を用いてxを変更するとyも一緒に変更されてしまうことに注意が必要です。これはadd_()がin-placeなメソッドであることが理由です。in-placeな方法による変更が難しいと感じるのであれば、not in-placeなadd()を用いても良いかもしれません。その他のin-placeな記述で有名なものとしてはy+=xのように「+=」を用いた記述方法があります。「y=y+x」と「y+=x」は同じことであるという文献を多々見ますが、厳密には異なる方法であるということを頭に入れておくと良いと思います。<br><br>\n","\n","※in-placeとは？<br>\n","x.add_(1)やy+=xのように関数の適用や値の代入などを行う際，元のオブジェクトxの要素も変更されること。"],"metadata":{"id":"2iEJiMB5uUH2"}},{"cell_type":"code","source":["#in-placeなadd_()メソッドを用いた場合\n","x= torch.tensor([1,2,3,4,5,6])\n","y = x.numpy() #TensorをNumpyに変換\n","x.add_(1)\n","print(\"x:\",x)\n","print(\"y:\",y)"],"metadata":{"id":"fRwXswF8tsU_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026073693,"user_tz":-540,"elapsed":6,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"7b718d66-3235-4de6-b826-b9d84740690a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor([2, 3, 4, 5, 6, 7])\n","y: [2 3 4 5 6 7]\n"]}]},{"cell_type":"code","source":["#in-placeなadd()メソッドを用いた場合\n","x= torch.tensor([1,2,3,4,5,6])\n","y = x.numpy() #TensorをNumpyに変換\n","x = x.add(1)\n","print(\"x:\",x)\n","print(\"y:\",y)"],"metadata":{"id":"y7OER39ByJxs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026073694,"user_tz":-540,"elapsed":5,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"ea89de70-7fd5-4576-cbb7-522c6365ca9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor([2, 3, 4, 5, 6, 7])\n","y: [1 2 3 4 5 6]\n"]}]},{"cell_type":"code","source":["#補足　「x+=y」の形式を用いた場合\n","x= torch.tensor([1,2,3,4,5,6])\n","y = x.numpy() #TensorをNumpyに変換\n","x+=1\n","print(\"x:\",x)\n","print(\"y:\",y)"],"metadata":{"id":"0xnly9q8CTqf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026074460,"user_tz":-540,"elapsed":6,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"f6da5273-ab23-4c23-bc16-fb78d875ab72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor([2, 3, 4, 5, 6, 7])\n","y: [2 3 4 5 6 7]\n"]}]},{"cell_type":"code","source":["#補足　「x=x+y」の形式を用いた場合\n","x= torch.tensor([1,2,3,4,5,6])\n","y = x.numpy() #TensorをNumpyに変換\n","x = x+1\n","print(\"x:\",x)\n","print(\"y:\",y)"],"metadata":{"id":"zb2MPRK-CeVQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026074460,"user_tz":-540,"elapsed":5,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"cae71177-b14e-47f6-ab23-fc6bc51ba08a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor([2, 3, 4, 5, 6, 7])\n","y: [1 2 3 4 5 6]\n"]}]},{"cell_type":"markdown","source":["## NumPyからTensorへの変換\n","NumPyをTensorに変換するには、from_numpy()メソッドを用います。"],"metadata":{"id":"66tvpONX-kfh"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","x= np.array([1,2,3,4,5,6])\n","y = torch.from_numpy(x)\n","print(x)\n","print(type(x))\n","print(y)\n","print(y.type())"],"metadata":{"id":"5_AsGgLBCzl2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026475756,"user_tz":-540,"elapsed":607,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"228eea77-b97e-4f1e-f8a5-e21e5416ca0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 2 3 4 5 6]\n","<class 'numpy.ndarray'>\n","tensor([1, 2, 3, 4, 5, 6])\n","torch.LongTensor\n"]}]},{"cell_type":"markdown","source":["尚、一般的にPyTorchではtorch.floatを用います。そのため、以下のようにnumpyでfloat32にキャストしてから利用する方法を覚えておくと良いでしょう。"],"metadata":{"id":"FlPkkvgjJy_u"}},{"cell_type":"code","source":["x= np.array([1,2,3,4,5,6])\n","y = torch.from_numpy(x.astype(np.float32))\n","print(x)\n","print(type(x))\n","print(y)\n","print(y.type())"],"metadata":{"id":"w7xAjFO8EgQh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026475756,"user_tz":-540,"elapsed":5,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"1c142caf-5753-42e1-e6c5-dcbcb4e3c781"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 2 3 4 5 6]\n","<class 'numpy.ndarray'>\n","tensor([1., 2., 3., 4., 5., 6.])\n","torch.FloatTensor\n"]}]},{"cell_type":"markdown","source":["# 4. GPUとCPUの使い分け\n","4章では、PyTorchにおいてGPUを用いる際に必要となるGPUの指定方法について説明します。<br>\n","GPUを使用する前に、まず以下のようにGPUが搭載されているが、どのGPUが用いられているのか確認します。"],"metadata":{"id":"37bgzuNqe0O2"}},{"cell_type":"code","source":["import torch\n","if torch.cuda.is_available():\n","  print(f'GPUデバイス数： {torch.cuda.device_count()}')\n","  print(f'現在のGPUデバイス番号： {torch.cuda.current_device()}')\n","  print(f'1番目のGPUデバイス名： {torch.cuda.get_device_name(0)}')\n","else:\n","  print('GPU使用不可')"],"metadata":{"id":"IBYS8lUfzlLR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026543966,"user_tz":-540,"elapsed":807,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"4255f792-95f1-42ae-bc55-212dd2b8c5fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPUデバイス数： 1\n","現在のGPUデバイス番号： 0\n","1番目のGPUデバイス名： Tesla T4\n"]}]},{"cell_type":"markdown","source":["このように、Google CoraboratoryではNVIDIA Teslaが使用できることが分かります。<br>\n","これを踏まえ、GPUを明示的に使用する方法を見ていきます。まず、以下のようにdeviceという変数にCPU、GPUどのデバイスを用いるかを格納します。"],"metadata":{"id":"Enyc_SMVewtp"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #デバイスとしてGPU(cuda)かCPUか判定\n","print(\"デバイスの種類：\",device)"],"metadata":{"id":"C6pAbsYjLc9N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026550215,"user_tz":-540,"elapsed":456,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"f875f788-c5bc-46b9-f021-d1d581d628ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["デバイスの種類： cuda\n"]}]},{"cell_type":"markdown","source":["このように、deviceという変数にデバイスの種類を格納しました。そして、GPU上でテンソル計算を行うためには、\n","\n","- テンソルをdeviceを指定してGPU上で生成\n","- テンソルをdeviceを指定してGPU上に送る\n","  - to(device)で移動\n","\n","のどちらかを行う必要があります。上記の2つは以下のように実現することができます。"],"metadata":{"id":"yYJP-x-YL8vV"}},{"cell_type":"code","source":["#GPU上でテンソル作成\n","x = torch.ones(2,3,device = device) #deviceを指定してGPU上でテンソル作成\n","print(x)"],"metadata":{"id":"J3nZglrUL6F7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026552775,"user_tz":-540,"elapsed":6,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"52a9157e-5025-451a-c363-8a925f35e0cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["#CPU上でテンソル作成、GPUに送る\n","x = torch.ones(2,3) #CPU上でテンソル作成\n","print(x,x.device)\n","x = x.to(device) #GPUにテンソル送る\n","print(x)"],"metadata":{"id":"VnYLdcDQNM3Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026552776,"user_tz":-540,"elapsed":5,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"a1b05ca9-0126-4d49-e589-f75b2b6300cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) cpu\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["このように、テンソルをデバイス（CPU、GPU）を明示して作成することで、用いるデバイスを容易に切り替えることができます。<br>\n","そのため、CPU演算で十分の場合にはCPU、学習などGPU演算により効率性の上がるものはGPUを指定することで、効率的な処理が実現できます。<br>\n","尚、GPUに生成されたテンソルを取り出しndarray形式に変換する際には注意が必要です。先に説明した方法でndarray形式に変更しようとすると、以下のようなエラーが生じます。"],"metadata":{"id":"529qfi-MN6ay"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #デバイスとしてGPU(cuda)かCPUか判定\n","x = torch.ones(2,3) #CPU上でテンソル作成\n","x = x.to(device) #GPUにテンソル送る\n","y = x.numpy()\n","print(y,type(y))"],"metadata":{"id":"kPMOGXXGN4xP","colab":{"base_uri":"https://localhost:8080/","height":226},"executionInfo":{"status":"error","timestamp":1651026572483,"user_tz":-540,"elapsed":448,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"15a78a61-f279-4aff-be80-679d5faed1bc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-7648a22e18a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#CPU上でテンソル作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#GPUにテンソル送る\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}]},{"cell_type":"markdown","source":["このようなエラーを解消するためには、以下のように`to(\"cpu\").detach().numpy()`と記述することで、CPUに送ることを明示的に指定して取り出す必要があります。"],"metadata":{"id":"2kohr3yMtJNC"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #デバイスとしてGPU(cuda)かCPUか判定\n","x = torch.ones(2,3) #CPU上でテンソル作成\n","x = x.to(device) #GPUにテンソル送る\n","y = x.to(\"cpu\").detach().numpy()\n","print(x,x.type())\n","print(y,type(y))"],"metadata":{"id":"CCBg-NS-pwFV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651026574635,"user_tz":-540,"elapsed":4,"user":{"displayName":"福井稔基","userId":"10660958909021026951"}},"outputId":"453f361d-efe3-4d1d-b76d-4f04c25e537d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0') torch.cuda.FloatTensor\n","[[1. 1. 1.]\n"," [1. 1. 1.]] <class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","source":["※ 本資料は、[Pytorch公式のチュートリアル](https://yutaroogawa.github.io/pytorch_tutorials_jp/)を参考に作成致しました。\n","本資料には記載していない、さらに詳しい内容も公式チュートリアルには記載されていますので、興味ある方は調べてみてください。\n"],"metadata":{"id":"emEDwhtutI7e"}}]}